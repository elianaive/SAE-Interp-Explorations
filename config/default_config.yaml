# Model configuration
model:
  name: "roneneldan/TinyStories-33M"
  layer_num: 3

# Data configuration
data:
  name: "roneneldan/TinyStories"
  train_size: 100000
  sequence_length: 64
  extraction_batch_size: 128
  activation_cache_dir: "data/activations"

# Autoencoder configuration
autoencoder:
  latent_dim: 8192 #similar to GPT 2
  k: 32
  init_scale: 2.0
  tied_weights: true
  multi_k: true

# Training configuration
training:
  num_epochs: 100
  batch_size: 12
  learning_rate: 1e-4
  adam_eps: 6.25e-10
  k_aux: 384 # Should be d_model/2 ish
  aux_scale: 0.03125  # 1/32
  num_workers: 4
  checkpoint_dir: "checkpoints"
  patience: 5
  max_dead_latent_ratio: 20.0

# Wandb configuration
use_wandb: true
wandb:
  project_name: "SparseAutoencoder"
  dir: "wandb"