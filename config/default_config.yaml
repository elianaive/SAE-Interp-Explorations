# Model configuration
model:
  name: "roneneldan/TinyStories-33M"
  layer_num: 3

# Data configuration
data:
  name: "roneneldan/TinyStories"
  train_size: 50000 #100000000
  sequence_length: 256
  extraction_batch_size: 64
  activation_cache_dir: "data/activations"
  chunks_to_keep: 2
  chunk_size: 8192
  num_workers: 0

# Autoencoder configuration
autoencoder:
  latent_dim: 12288 # Expansion factor of 16
  k: 32
  init_scale: 0.177  # 1/sqrt(k) where k=32
  tied_weights: true
  multi_k: false

# Training configuration
training:
  num_epochs: 50
  batch_size: 128
  learning_rate: 2e-4 #1e-4
  adam_eps: 6.25e-10
  k_aux: 384 # Should be d_model/2 ish
  aux_scale: 0.03125  # 1/32
  num_workers: 4
  checkpoint_dir: "checkpoints"
  patience: 5
  max_dead_latent_ratio: 20.0
  total_tokens: 1_000_000_000  # Total tokens to process
  tokens_per_checkpoint: 50_000_000  # How often to checkpoint/evaluate
  sequence_length: 256
  eval_tokens: 100_000 

# Wandb configuration
use_wandb: true
wandb:
  project_name: "SparseAutoencoder"
  dir: "wandb"